{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkContext\n",
    "\n",
    "Main entry point for Spark functionality. A SparkContext represents the connection to a Spark cluster, and can be used to create RDDs, accumulators and broadcast variables on that cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkSession\n",
    "The entry point to programming Spark with the DataFrame API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD\n",
    "A Resilient Distributed Dataset (RDD), the basic abstraction in Spark. \n",
    "* immutable, partitioned collection of elements that can be operated on in parallel. \n",
    "* Optimized for Scala\n",
    "* Track lineage information to efficiently recompute lost data\n",
    "\n",
    "You construct RDDs\n",
    "* by parallelizing existing collections\n",
    "* by transforming ane existing RDD\n",
    "* by reading files from storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame\n",
    "The preferred abstraction in Spark\n",
    "* Strongly typed collection of distributed elements\n",
    "  * Built on Resilient Distributed Datasets\n",
    "* _Immutable once constructed_\n",
    "* Track lineage information to efficiently recompute lost data\n",
    "* Enable operations on collection of elements in parallel\n",
    "* Inspired by data frames in R and Python (Pandas)\n",
    "\n",
    "You construct DataFrames\n",
    "* by parallelizing existing collections (e.g., Pandas DataFrames) \n",
    "* by transforming an existing DataFrame\n",
    "* by reading files from storage\n",
    "\n",
    "Citation: Databricks main page (https://databricks.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Spark Cluster Architecture\n",
    "![](http://spark.apache.org/docs/1.1.0/img/cluster-overview.png)\n",
    "* Master node and one or more slaves nodes. \n",
    "* The master node runs the driver program (your actual application) and distributes the work that needs to be done to the so called worker nodes (the slaves). \n",
    "* Within a worker node one or more executors can run. \n",
    "* An executor is a process that runs the actual computations (the tasks distributed by the master). Note that the executor has, in general, one or more cores available for performing its tasks.\n",
    "* SparkContext can connect to several types of cluster managers (either Sparkâ€™s own standalone cluster manager or Mesos/YARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.2 |Anaconda 4.2.0 (64-bit)| (default, Jul  2 2016, 17:53:06) \n",
      "[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\n",
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create RDDs from collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arrayRDD = spark.sparkContext.parallelize(range(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrayRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations\n",
    "create a new dataset from an existing one, and actions, which return a value to the driver program after running a computation on the dataset.\n",
    " \n",
    "`glom()` returns an RDD created by coalescing all elements within each partition into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arrayRDDByPartition = arrayRDD.glom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[6] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrayRDDByPartition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action\n",
    "\n",
    "Actions are RDD operations that produce non-RDD values. They materialize a value in a Spark program. In other words, an RDD operation that returns a value of any type but `RDD[T]` is an action.\n",
    "\n",
    "`collect()` is an action, but all the data should fit on memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1],\n",
       " [2, 3, 4],\n",
       " [5, 6],\n",
       " [7, 8, 9],\n",
       " [10, 11],\n",
       " [12, 13, 14],\n",
       " [15, 16],\n",
       " [17, 18, 19]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrayRDDByPartition.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pi Estimation with Resilient Distributed Datasets (RDDs)\n",
    "\n",
    "A simple Monte Carlo simulation to approximate the value of  $\\pi$  could involve randomly selecting points  ${(x_i,y_i)}_{i=1}^n$  in the unit square and determining the ratio  $f=\\frac{m}{n}$,  where $m$ is number of points that satisfy  $x_i^2+y_2^2\\leq1$.\n",
    "\n",
    "Therefore, we need to define the sample size$n$, which in this case we assume 10,000,000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#define sample number, and distribute the array using parallelize\n",
    "numSamples = 10000000\n",
    "samplesDF = spark.sparkContext.parallelize(range(numSamples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show the first 10 values\n",
    "samplesDF.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping in Spark\n",
    "`map( f(x) )` is a transformation that passes each dataset element through a function and returns a new RDD representing the results.\n",
    "![](https://www.sharcnet.ca/~jnandez/map_pi.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generate the Monte Carlo simulation\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "def circle(value):\n",
    "    x, y = random.random()*2 - 1, random.random() * 2 - 1, #find a random number for x and y between -1 and 1\n",
    "    return 1 if x*x + y*y <=1 else 0 #check if x*x+y*y<=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mapSamplesDF = samplesDF.map(circle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 0, 0, 1, 1, 1, 1, 1, 0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapSamplesDF.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing the RDD\n",
    "`reduce` is an action that aggregates all the elements of the RDD using some function and returns the final result to the driver program.\n",
    "\n",
    "![](https://www.sharcnet.ca/~jnandez/reduce_pi.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = mapSamplesDF.reduce(lambda x,y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7851392"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of PI is  3.1405568\n"
     ]
    }
   ],
   "source": [
    "print(\"The value of PI is \",4.0 * count / numSamples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Word Count in Spark\n",
    "\n",
    "This example uses `DataFrame` since it is the most popular API in Spark. \n",
    "\n",
    "Goal: Read a (unstructured) text files and determine which words are more common in those files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_data = spark.read.text(\"data/pg*.txt\") #creates a dataframe when the files is read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`show()` is an action that displays 20 records by default in a short version, you can define the number of record and if you want to truncate the records or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------+\n",
      "|value                                                                       |\n",
      "+----------------------------------------------------------------------------+\n",
      "|The Project Gutenberg EBook of The Complete Works of William Shakespeare, by|\n",
      "|William Shakespeare                                                         |\n",
      "|                                                                            |\n",
      "|This eBook is for the use of anyone anywhere at no cost and with            |\n",
      "|almost no restrictions whatsoever.  You may copy it, give it away or        |\n",
      "+----------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark contains many functions that can manipulate columns. Check this page\n",
    "\n",
    "* https://spark.apache.org/docs/2.1.1/api/java/org/apache/spark/sql/functions.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower,col\n",
    "lowerDF = raw_data.select( lower(col('value')) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`col()` returns a `Column` based on the given column name. Otherwise, you will have to do,\n",
    "\n",
    "\n",
    "`lowerDF = raw_data.select( lower( raw_data['value'] ) )`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------+\n",
      "|lower(value)                                                                |\n",
      "+----------------------------------------------------------------------------+\n",
      "|the project gutenberg ebook of the complete works of william shakespeare, by|\n",
      "|william shakespeare                                                         |\n",
      "|                                                                            |\n",
      "|this ebook is for the use of anyone anywhere at no cost and with            |\n",
      "|almost no restrictions whatsoever.  you may copy it, give it away or        |\n",
      "+----------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lowerDF.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------+\n",
      "|value                                                                       |\n",
      "+----------------------------------------------------------------------------+\n",
      "|the project gutenberg ebook of the complete works of william shakespeare, by|\n",
      "|william shakespeare                                                         |\n",
      "|                                                                            |\n",
      "|this ebook is for the use of anyone anywhere at no cost and with            |\n",
      "|almost no restrictions whatsoever.  you may copy it, give it away or        |\n",
      "+----------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lowerDF = raw_data.select( lower(col('value')).alias('value') ) #alias() function changes the name of the Column\n",
    "lowerDF.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regular Expressions in Spark\n",
    "`regexp_replace(Column e, Column pattern, Column replacement)` replaces all substrings of the specified string value that match regexp with rep. \n",
    "\n",
    "A [regular expression](https://en.wikipedia.org/wiki/Regular_expression) is a sequence of characters that define a search pattern. \n",
    "\n",
    "\n",
    "Check this website for a course in regular expressions:\n",
    "\n",
    "* http://v4.software-carpentry.org/regexp/operators.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------+\n",
      "|value                                                                      |\n",
      "+---------------------------------------------------------------------------+\n",
      "|the project gutenberg ebook of the complete works of william shakespeare by|\n",
      "|william shakespeare                                                        |\n",
      "|                                                                           |\n",
      "|this ebook is for the use of anyone anywhere at no cost and with           |\n",
      "|almost no restrictions whatsoever  you may copy it give it away or         |\n",
      "+---------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "noPunctuationDF = lowerDF.select( regexp_replace(col('value'),'\\p{P}','').alias('value') )\n",
    "noPunctuationDF.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------+\n",
      "|value                                                                      |\n",
      "+---------------------------------------------------------------------------+\n",
      "|the project gutenberg ebook of the complete works of william shakespeare by|\n",
      "|william shakespeare                                                        |\n",
      "|                                                                           |\n",
      "|this ebook is for the use of anyone anywhere at no cost and with           |\n",
      "|almost no restrictions whatsoever  you may copy it give it away or         |\n",
      "+---------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import trim #Trim the spaces from both ends for the specified string column. \n",
    "\n",
    "noEndSpaceDF = noPunctuationDF.select( trim(col('value')).alias('value') )\n",
    "noEndSpaceDF.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our sentences are clean, we can get each word in each sentence. This task is done by splitting the sentence using the blank spaces. We use the function\n",
    "\n",
    "`split(Column str, String pattern)` splits str around pattern (pattern is a regular expression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------+\n",
      "|words                                                                                   |\n",
      "+----------------------------------------------------------------------------------------+\n",
      "|[the, project, gutenberg, ebook, of, the, complete, works, of, william, shakespeare, by]|\n",
      "|[william, shakespeare]                                                                  |\n",
      "|[]                                                                                      |\n",
      "|[this, ebook, is, for, the, use, of, anyone, anywhere, at, no, cost, and, with]         |\n",
      "|[almost, no, restrictions, whatsoever, you, may, copy, it, give, it, away, or]          |\n",
      "+----------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "\n",
    "splitValueDF = noEndSpaceDF.select( split(col(\"value\"),'\\W+').alias(\"words\") ) \n",
    "#\\w+ matches one or more word characters\n",
    "splitValueDF.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|word     |\n",
      "+---------+\n",
      "|the      |\n",
      "|project  |\n",
      "|gutenberg|\n",
      "|ebook    |\n",
      "|of       |\n",
      "+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "#explode() creates a new row for each element in the given array or map column. \n",
    "wordsDF = splitValueDF.select( explode(col(\"words\")).alias('word') )\n",
    "wordsDF.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can count how many words we have in the `DataFrame` using the action `count()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1184180"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our long list of words, some are very likely to be repeated. If we want to know how many times a word repeats in the `DataFrame`, we need to group by word, and then count them. We use `groupBy()` function to group by `word`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "groupedWordsDF = wordsDF.groupBy(\"word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GroupedData' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-5da2e06c9b86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgroupedWordsDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'GroupedData' object has no attribute 'show'"
     ]
    }
   ],
   "source": [
    "groupedWordsDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What does this means?\n",
    "[GroupedData](https://spark.apache.org/docs/1.6.2/api/java/org/apache/spark/sql/GroupedData.html#GroupedData(org.apache.spark.sql.DataFrame,%20scala.collection.Seq,%20org.apache.spark.sql.GroupedData.GroupType)) type is not a `DataFrame` is a set of methods for aggregations on a DataFrame, created by DataFrame.groupBy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x7f74e24d0748>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupedWordsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-----+\n",
      "|word                  |count|\n",
      "+----------------------+-----+\n",
      "|online                |24   |\n",
      "|those                 |722  |\n",
      "|some                  |1799 |\n",
      "|726002026compuservecom|1    |\n",
      "|art                   |925  |\n",
      "+----------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we can use the count() aggregation method and create a DataFrame\n",
    "groupedWordsDF.count().show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc # we want to have the result in a descending order.\n",
    "topWordsDF = groupedWordsDF.count().orderBy(desc(\"count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "|the |39697|\n",
      "|and |35054|\n",
      "|to  |27325|\n",
      "|i   |27058|\n",
      "|of  |26126|\n",
      "|a   |19454|\n",
      "|you |16922|\n",
      "|    |15859|\n",
      "|my  |15254|\n",
      "|in  |15071|\n",
      "+----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topWordsDF.show(10,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to shrink what we did?\n",
    "\n",
    "It is not difficult to shrink what we just did in few lines of code. Let's start by defining a `Python` function for removing the punctuation. Then get the wordDF, and finally give the final result. Note that we forgot to remove all white spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split,explode,lower,trim,regexp_replace,col,udf\n",
    "def removePunctuation(column):\n",
    "    return lower( trim( regexp_replace(column,\"\\p{P}\",'') ) ).alias(\"sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordsDF = (raw_data.select( explode(split(removePunctuation('value'),'\\W+')).alias(\"word\"))\n",
    "                  .filter(col(\"word\")!= '')\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "| the|39697|\n",
      "| and|35054|\n",
      "|  to|27325|\n",
      "|   i|27058|\n",
      "|  of|26126|\n",
      "|   a|19454|\n",
      "| you|16922|\n",
      "|  my|15254|\n",
      "|  in|15071|\n",
      "|that|14495|\n",
      "+----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(wordsDF.groupBy(\"word\")\n",
    "        .count()\n",
    "        .orderBy(desc(\"count\"))\n",
    " ).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: What are the least common words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|           word|count|\n",
      "+---------------+-----+\n",
      "|       mockable|    1|\n",
      "|         slaver|    1|\n",
      "|          bushs|    1|\n",
      "|      cleopatpa|    1|\n",
      "|       jugglers|    1|\n",
      "|      soundness|    1|\n",
      "|distemperatures|    1|\n",
      "|     unswayable|    1|\n",
      "|  fortuneteller|    1|\n",
      "|       forspoke|    1|\n",
      "+---------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(wordsDF.groupBy(\"word\")\n",
    "        .count()\n",
    "        .orderBy(\"count\")\n",
    " ).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# removing StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/stopwords.txt\",\"r\") as f:\n",
    "    stopwords = set([line.strip() for line in f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removeStopWords(tokens):\n",
    "    return [token for token in tokens if token not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'learning', 'Spark', 'COSS']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testSentence = \"I am learning Spark in the COSS\"\n",
    "removeStopWords(testSentence.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType,StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "splitDF = raw_data.select( split(removePunctuation('value'),'\\W+').alias(\"word\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------+\n",
      "|word                                                                                    |\n",
      "+----------------------------------------------------------------------------------------+\n",
      "|[the, project, gutenberg, ebook, of, the, complete, works, of, william, shakespeare, by]|\n",
      "|[william, shakespeare]                                                                  |\n",
      "|[]                                                                                      |\n",
      "|[this, ebook, is, for, the, use, of, anyone, anywhere, at, no, cost, and, with]         |\n",
      "|[almost, no, restrictions, whatsoever, you, may, copy, it, give, it, away, or]          |\n",
      "+----------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "splitDF.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "removeStopWordsUDF = udf(removeStopWords,ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "explodedNoStopWordsDF = (splitDF.select( explode(removeStopWordsUDF(\"word\")).alias(\"word\") )\n",
    "                                .filter(col(\"word\")!=\"\")\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     word|\n",
      "+---------+\n",
      "|  project|\n",
      "|gutenberg|\n",
      "|    ebook|\n",
      "| complete|\n",
      "|    works|\n",
      "+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "explodedNoStopWordsDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "| word|count|\n",
      "+-----+-----+\n",
      "| thou| 5499|\n",
      "|  thy| 4050|\n",
      "|shall| 3913|\n",
      "|  now| 3257|\n",
      "| thee| 3188|\n",
      "| good| 3110|\n",
      "| lord| 3079|\n",
      "| king| 2933|\n",
      "|  sir| 2876|\n",
      "| well| 2856|\n",
      "+-----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "explodedNoStopWordsDF.groupBy(\"word\").count().orderBy(desc(\"count\")).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Exercise: What are the least common words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|        word|count|\n",
      "+------------+-----+\n",
      "|       shank|    1|\n",
      "|      papist|    1|\n",
      "|pibblepabble|    1|\n",
      "|antipholuses|    1|\n",
      "|   registers|    1|\n",
      "|     designd|    1|\n",
      "|        mede|    1|\n",
      "|     porches|    1|\n",
      "|  pastsaving|    1|\n",
      "|       exion|    1|\n",
      "+------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(explodedNoStopWordsDF.groupBy(\"word\")\n",
    "        .count()\n",
    "        .orderBy(\"count\")\n",
    " ).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
